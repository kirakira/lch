\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage{epstopdf}
\usepackage{titlesec}
\usepackage{amssymb,amsmath}
\usepackage{amsthm}
%\usepackage{subfigure}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage[]{algorithm2e}
\usepackage{xcolor,listings}
\usepackage{multirow}
\usepackage{hyperref}
\newtheorem{thm}{Theorem}

\setcounter{secnumdepth}{4}
\titleformat{\paragraph}
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titlespacing*{\paragraph}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

\title{LCH: A Reliable Distributed Source Code Version Control System}
\author{Shike Mei, Menghui Wang, and Lichao Yin}

\begin{document}
\maketitle

\abstract{
    Today popular source code version control systems do not replicate data for reliability.
    We share our experience in building LCH (abbreviated from Launch), a reliable source code version control system.
    LCH replicates data on multiple servers and uses the Paxos\ref{paxos} protocol along with an update log to provide strong consistency to clients.
    With a three-step catch-up mode, LCH can also recover from failures seamlessly.
    Our test results show LCH is a reliable version control system.
}

\section{Introduction}

A version control system is a system that keeps track of multiple history versions of every managed document, and allows a user to checkout or revert to previous versions as needed.
Version control systems usually allow for a group of people contributing to one project.
Today version control systems have an extensive use in source code management, collaborative editing, data backup, etc.

Due to different applications, a version control system can differ from another in many aspects.
\begin{itemize}
    \item History granularity: linear versus directed acyclic graph (DAG).
        Examples include: Google Docs (linear), Subversion (linear), git (DAG), Mercurial (DAG).
    \item Implicit checkpointing versus explicit checkpointing.
        An implicit checkpointing system will automatically save the state of the current document whenever the system thinks appropriate, while an explicit checkpointing system will not do so until instructed by the user.
        Examples include: Google Docs (implicit), Dropbox (implicit), git (explicit), Subversion (explicit).
    \item Conflict Resolution.
        A conflict is when multiple versions of a same file (or a same line of a same file) are found at the same time.
        Conflicts could result from out-of-date repository, conflicting edits from different sources, network delays, or system faults.
        Some systems will resolve conflicts automatically based on some metrics or mechanisms.
        Some examples include conflict resolution by timestamp (Google Contacts), vector clocks.
        Other systems will need human intervention to resolve conflicts (Dropbox, git, Subversion, CVS).
    \item Failure model.
        Some systems have a centralized server and thus are vulnerable to single-node failures (Subversion, CVS, git, Mercurial\footnotemark),
        while others will store data reliably on a distributed set of servers (Google Docs).
        \footnotetext{
        Although git and Mercurial are capable of storing data on multiple servers, replication is not supported natively and users will need to push data to every single place manually.
        Moreover, since atomicity is not provided when doing so, it may will cause complex consistency issues.
    For this reason, most git and Mercurial users will use git and Mercurial in the same way as they use Subversion: designate a central server to store all the data.}
    \item Offline edits.
        Some systems allow making modifications while offline (git, Dropbox) while others do not (Subversion, Google Docs).
\end{itemize}

Replication is the key to reliability.
However from the list above one can see that today few version control system replicates data.
This motivates our project.
Simply speaking, LCH is a version control system that will store source code reliably on a distributed set of servers.
Source code is different from other application in that it needs to be precise at any time.
Thus strong consistency needs to be preserved while replicating data.
Paxos\ref{paxos} is a protocol that solves the consensus problem in a distributed system.
With the help of an update log, we can build a strongly consistent distributed system that replicates data across many servers.

LCH is designed to tolerate failures.
When a failed server is restarted, it will first operate in a 3-step catch-up mode.
Unlike the catch-up mode proposed by Google \ref{paxos2}, we add an additional third step that will make sure everything missed out is collected.
The catch-up mode will be discussed in detail in Section~\ref{sec:impl}.

In the original protocol, it is possible that Paxos rounds keep failing because multiple proposers make conflicting proposes in an alternating way.
We propose random backoff to mitigate this issue.
In our solution, when such scenario is detected, a proposer will, with a certain probability, try to postpone its proposal for a random period of time, so as to minimize the likelihood that more than one proposers propose at the same time.

In the rest of this paper we discuss the various trade-offs in when we design LCH, and we share our experience in implementing it.
It is organized as follows: in Section~\ref{sec:design}, we discuss our reasonings and considerations behind the various decisions we made in designing LCH.
Section~\ref{sec:impl} will mainly focus on implementation details of the server and the client, including how we incorporate the Paxos protocol in our system and some improvements of it.
Section~\ref{sec:eval} will state our methodology in evaluating our system.
Section~\ref{sec:conclusion} concludes our project.


\section{Design}
\label{sec:design}
In this section we will discuss the design of LCH, as well as our reasonings behind every decision.

\subsection{System Overview}
LCH is a client-server application.
Unlike a traditional version control system, user data is replicated on multiple LCH servers for reliability.
Figure~\ref{fig:1} shows a typical topology structure of LCH.

\subsection{Distributed Servers}
LCH is a distributed system.
Multiple servers can serve their own clients in the same time.
Every client can choose to connect to any LCH server, and servers will handle replication and consistency behind the scene.
Client will feel like there is only one server there.
That being said, when the server a client is connecting to is not responding, the client can choose to connect to any other server.
In contrast to our distributed design, an alternative is to use a master-slave scheme.
Although the master-slave scheme handles consistency issues more easily, it is less reliable than our distributed structure.
For example, when a master failure is monitored, a master flip must be done.
During the flip, there is a short period of time that the system is not available.
Moreover, a master failure may also cause data loss.

\subsection{Client Design}
LCH uses similar terminologies and concepts as were used in other version control systems.
A version is a state consisting of the states of all managed files.
Changes made to a version are called a commit, and a commit will lead to a new version.
LCH provides linear version history, which means commits can only be made to the latest version.
As a consequence, every version is associated with a single integral number, beginning from $0$, which is the version for an empty repository.
It is worth noting that although we choose to implement LCH using linear version history, DAG version history can also be achieved under our design.
We choose to use linear version history because it provides a cleaner and more understandable interface to the user, and it is less likely for misoperations to happen.
Though a DAG-based history is more powerful and flexible, it is also harder to implement and we choose not to spend our efforts on it.

The client offers two simple commands, \texttt{sync} and \texttt{commit}.
\texttt{sync} will pull from a server recent changes from the current version of the client, and then the client will apply all those changes.
A \emph{conflict} may happen when both remote and local changes are made to the same file.
In this case, the remote change will not be applied, and a copy of the remote file will be created.
Then the user will be directed to resolve the conflict.
We choose to let the user to resolve conflicts for the following two reasons:
\begin{itemize}
    \item The conflicts happen in LCH are not easily resolvable by a machine or by certain rules.
        A conflict usually involves two uncomparable versions due to modifications made on an outdated repository.
        Consolidating them requires knowledge beyond the scope of the system.
    \item Human has a better understanding of what the code is doing, and is more capable of resolving conflicts.
\end{itemize}

\texttt{commit} command will scan the all the local files, and compute the changes made to the current local version.
Then it will push these changes to the remote server.
There are two possible outcomes: accepted or rejected.
If the commit is accepted, then the local version is advanced by $1$.
Otherwise, it must be because the local version is out-dated.
Thus the user must run the \texttt{sync} command first.
The main reason LCH uses explicit checkpointing is that we feel it is better to let user to organize each commit because user has a better understanding of what the code does.

\subsubsection{Online Commits}
LCH requires that all commits must be submitted to servers at the time they are made.
Therefore no offline commits are allowed.
This is a consequence of our previous designs,
\begin{itemize}
    \item Since it is possible that a commit be rejected by the server, the client must contact the server to know if this is the case.
    \item If we allow a commit to be staged but not submitted, it will cause more consistency issues.
        Suppose one of the staged commit turns out to be rejected by the server.
        What should we do to its following commits?
        There is no easy way we can handle this under our linear history model.
\end{itemize}

\subsubsection{A Typical Workflow}
Figure~\ref{fig:2} shows a typical use case of LCH.
The user first run a \texttt{sync} command to sync its local repository.
The user then make modifications to its local repository until it is ready for a checkpoint.
Then a \texttt{commit} command will be run, and the client will notify if the commit is accepted or not.
If the commit is accepted, the user can keep making other changes.
Otherwise, the user must run a \texttt{sync} command to pull latest changes from the server.
Conflicts may happen during this step, and if this is the case, the user must resolve conflicts given the remote file and the local file.
When conflicts are resolved, the user can run another \texttt{commit} command to submit previous changes (along with the conflict resolution).

\subsection{Full Replication}
Every LCH server is identical and equally capable.
This means, every LCH server will store a whole database of all history commits.
We do not choose to partition our data for the following reasons:
\begin{itemize}
    \item Full replication provides higher read throughput than partitioning does.
        In our current design, every server is able to serve sync requests clients in parallel
        (not true for commit requests because they need to go through a Paxos round).
        If we partition our data, every server will only be able to serve requests concerning the portion of data it has.
    \item As for a version control system for source code, it is unlikely that the size of a single repository will exceeds the limit of a single machine.
        Therefore there is no need to replicate data for capacity as well.
\end{itemize}

\subsection{Strong Consistency}
When designing our consistency model, our criteria is ``every commit submitted must base on the things that the user has already \emph{seen}'', because source code must be precise at all times.
Therefore we choose to maintain strong consistency and let user to resolve conflicts.
That is, every \texttt{sync} command will return the latest version by a most recent \texttt{commit}, and no commit shall be accepted unless it is based on the latest version.

Strong consistency is at the cost of reduced throughput.
To preserve strong consistency, every commit must go through a full Paxos round, which will involve all servers to participate.
As a result, only one commit will be accepted at a time among all servers.
However, as a tool for managing source code, we do not anticipate a very high concurrent commit rate.

\subsection{Fault-Tolerant and Recovery}
LCH tolerates non-Byzantine failures by replication.
When a server failed, since all the servers are equally capable, the client can choose to connect to any other servers with the same request.
As long as the total number of failed servers is less than half of the total, LCH will work normally.
This result is implied by the fault-tolerant characteristic of the Paxos protocol.
To see rigorously how replication improves the reliability, one can refer to Theorem~\ref{thm:1} in Appendix.

When a failed server is restarted, it must start in a catch-up mode.
The catch-up mode will help the failed server catch up all the information it missed during failures.
They are essentially the results of previous Paxos rounds.
Before it can serve in normal mode again, it must ensure that the data stored on it is synchronized with every other sever.
We will discuss the techniques used in the catch-up mode in detail in Section~\ref{sec:impl}.

\section{Implementation}
\label{sec:impl}
In this section we will discuss various implementation details when we build LCH.
We coded LCH from scratch in Java and the source code is available online\footnotemark.
\footnotetext{\url{https://github.com/kirakira/lch}.}

\subsection{Paxos and Update Log}
\label{sec:paxos}
Paxos is a consensus protocol introduced in \ref{paxos}.
It solves the following problem:
every server has its own proposal, and the group of servers want to agree on one of the proposals.
In LCH, every server will store a database containing all the accepted commits, and the database stored in every server should all be identical.
The Paxos protocol is used to select a request to modify the database.
Since all servers run in parallel, it is possible that at the same time multiple servers have received commit requests.
By our consistency requirement, only one commit can be based on the current version.
Then which commit request should we process?
This is when the Paxos protocol comes into play.
We will use the Paxos protocol to make all servers agree on one commit request to \emph{process}.

Though we select one commit request globally, the selected request is then being processed locally with the help of the update log.
The update log contains the results of all previous Paxos rounds, i.e. all the commits that were selected to process (but not necessarily to accept due to the existence of out-of-date commits).
After a Paxos round finishes, a new entry is first being added to the update log carrying the result of this Paxos round.
The newly added entry is marked as ``unfinished'' at this time.
It is first decided, according to the server's local database, whether this commit should be accepted based on the following criteria: if this commit is based on the current version, then it should be accepted; otherwise it should not.
If it is decided that the commit should be accepted, the server then applies the commit in the database.
Finally, the server modifies the corresponding entry in the update log to change its state to ``finished''.
If the current server happens to be the server that received the request, it will then notify the client the result of this commit.

\subsubsection{Implementation Details}
The original Paxos protocol has $3$ roles: proposer, acceptor, and learner.
We combined the latter two roles, and used $2$ threads each server doing the Paxos logic.
One thread will listen to client's \texttt{commit} requests and acts as the proposer.
The other thread will act as the acceptor and learner.

A global condition variable \texttt{roundFinish} will be used to synchronize the two threads.
\texttt{roundFinish} will be signaled by the acceptor and learner thread when a Paxos round has finished.

The activity of a proposer is initialized by an incoming commit request.
Only one commit request will be processed at the same time.
The proposer will first select a unique proposal number higher than any previous proposal numbers as follows:
it selects an integer $x$ greater than any previous proposal numbers and such that $x \equiv i \mod n$ where $n$ is the total number of proposers and $i$ is the $0$-based index of the current proposer.
It then starts to make proposals to every server (including itself) as specified by the Paxos protocol.
After all accept requests are sent out, it sleeps on the condition variable \texttt{roundFinish}, waiting for the result to be notified by the learner thread.
After being waken (or timed out after $10$ seconds), it checks the database to see if the request commit has been applied or not.
If everything goes well, the commit should pass the Paxos round and be learned by the learner thread (by writing to the update log and the database)
Finally the proposer will notify the result (accepted or not) to client.

The activity of the acceptor and learner thread is initialized by incoming Paxos packets (prepare, accept request, accepted, rejected).
The thread is implemented as a state machine, where the state includes the most recent promise \texttt{promise} made in this round, and the highest numbered proposal accepted \texttt{accepted}.
A round is regard as finish when more than half ``accepted'' are received for some proposal.
When a round is finished, \texttt{promise} and \texttt{accepted} are first cleared, and then the update log and the local database is updated atomically if necessary according to the selected commit request as is specified in \ref{sec:paxos}.
Finally, the condition variable \texttt{roundFinish} is signaled to wake all proposers sleeping on that.

\subsection{Catch-up Mode}
When a server is back from failure, it cannot start to serve immediately because of its out-of-date update log and database.


\subsection{Random Backoff}
\subsection{Client}

LCH provides users with a simple interface to maintain a \emph{linear} version history. Each version is called a commit, and an integer version number is associated with every commit. LCH provides two commands to work with: sync and commit. 
\begin{itemize}
  \item sync: pull changes from the server since the last commit in the local workset, and write those changes to corresponding local files. If a conflict is detected (both remote and local change the same place), user will be directed to resolve the conflict.
  \item commit: push local changes to the server as a new commit. The commit will be accepted if and only if the local version it is based on is up to date (i.e., last local commit matches the most recent commit on the server). If the commit is rejected, the user will be prompted to run a synccommand first.

\end{itemize}
A typical work flow of the client is as belows.
\begin{itemize}
  \item Initialization
  \item Make local changes
  \item sync
  \item Resolve conflicts
  \item commit
  \item Accepted? If yes, goto make local change; if no, goto sync
\end{itemize}

We note implementation details: First, when initialize the client, we need to set the directory for local copy and the path for metadata file. The metadata file stores the version information of latest sync from the server. Second, when doing sync, we first randomly ask a server to get the metadata and all changed information since the last sync version. Then, we compare the local changes on the last sync version and the changes of the latest version, if the two changes are different, we report a conflict file. For each conflict file, we update the file to latest server version, but make the copy the local changed version and warn user to resolve the conflict. Third, when doing commit, the client will first randomly ask a server the latest version, only when the versions are the same the commit can proceed. Send all changed files to the server, once the server returns ``successfully committed'', the client update the metadata file.

\section{Evaluation}
\label{sec:eval}
\subsection{Consistency Test}
\subsection{Reliability Test}
\subsection{Throughput Test}

\section{Conclusion}
\label{sec:conclusion}

\appendix
\section{A Proof that Replication Improves Reliability}
\begin{thm}
    \label{thm:1}
    Assuming that each server fails independently with probability $p$ $(p<0.5)$.
    If there are $2n+1$ servers, then the probability that more than $n$ servers fail simultaneously is less than $p$.
\end{thm}
\begin{proof}
    Let $S \subset \left\{1,\ldots,2n+1\right\}$ be the set of failed servers.
    The probability that $m$ out of $2n+1$ servers fail simultaneously (denote as $\Pr(|S|=m)$) is 
    \begin{eqnarray*}
        P(S=m) = \binom{2n+1}{m} p^{m} (1-p)^{2n+1-m},
    \end{eqnarray*}
    where $\binom{2n+1}{m}$ is the binomial coefficient.
    The probability that more than $n$ servers fails is denoted as $\Pr(|S|\geq n+1)$.
    \begin{eqnarray*}
        \Pr\left(|S|\geq n+1\right) = \sum_{m=n+1}^{2n+1} \binom{2n+1}{m} p^{m} (1-p)^{2n+1-m}.
    \end{eqnarray*}
    To prove that $\Pr(|S|\geq n+1) < p$, we first show prove that $\Pr(|S|\geq n+1)/p = 1$ 
    when $p=1/2$. It is because
    \begin{eqnarray*}
        P(|S|\geq n+1) &=& \sum_{m=n+1}^{2n+1} \binom{2n+1}{m} p^{m} (1-p)^{2n+1-m} \\ 
        (because~p=1/2)~&=& \sum_{m=n+1}^{2n+1} \binom{2n+1}{m} \left(\frac{1}{2}\right)^{2n+1} \\
        \left(because~\binom{2n+1}{m}=\binom{2n+1}{2n+1-m}\right)~&=& \frac{1}{2}\left(  \sum_{m=0}^{2n+1} 
        \binom{2n+1}{m} \left(\frac{1}{2}\right)^{2n+1} \right) \\
        &=& \frac{1}{2}.
    \end{eqnarray*}
    We then prove that the $P(|S|\leq n+1)/p$ is monotonously increasing with as $p$ 
    increasing (and $p<1/2$) by showing the derivative with respect to $p$ is positive
    \begin{eqnarray*}
        \frac{\mathrm{d}}{\mathrm{d}p}\Pr(|S|\geq n+1)/p &=& \sum_{m=n+1}^{2n+1} \binom{2n+1}{m} \left(\left(m-1\right)p^{m-2}
        \left(1-p\right)^{2n+1-m} \right.\\
        &&\left. - \left(2n+1-m\right)p^{m-1}\left(1-p\right)^{2n+1-m-1}\right) \\
        &=& \sum_{m=n+1}^{2n+1} \binom{2n+1}{m} \left(p^{m-2} 
        \left(1-p\right)^{2n+1-m-1}\left(m-1-2np\right)\right)\\
        (because~m-1-2np>0)&>& 0.
    \end{eqnarray*}
    So $\Pr(|S|\geq n+1)/p$ is increasing with $p$ when $p<0.5$.
    Since $\Pr(|S|\geq n+1)/p=1$ when $p=1/2$ and for $p<1/2$, $\Pr(|S|\geq n+1)/p$ is 
    increasing, $\Pr(|S|\geq n+1)/p < 1$ for all $p<1/2$. Therefore, the probability 
    that more than $n$ out of $2n+1$ servers fail simultaneously is smaller than 
    $p$.
\end{proof}


\section{References}

[1] Lamport, L. (2001). Paxos made simple. ACM Sigact News, 32(4), 18-25.

[2] Chandra, T. D., Griesemer, R., \& Redstone, J. (2007, August). Paxos made live: an engineering perspective. In Proceedings of the twenty-sixth annual ACM symposium on Principles of distributed computing (pp. 398-407). ACM.

\end{document}
